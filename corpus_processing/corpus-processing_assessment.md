---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---

# üèÜSelbsttest: Wissen und Praxis

````{admonition} Hinweis
:class: hinweis
Diese √úbungsaufgaben dienen Ihrer Selbsteinsch√§tzung und helfen Ihnen, das im Kapitel Gelernte zu reflektieren.

Sie k√∂nnen die Fragen in beliebiger Reihenfolge beantworten und auch mehrfach versuchen. 

**So funktioniert es:**
- W√§hlen Sie bei jeder Frage die Antwort(en), die Sie f√ºr richtig halten
- Lesen Sie das Feedback zu den einzelnen Antwortoptionen sorgf√§ltig durch
- Die Erkl√§rungen helfen Ihnen, Ihr Verst√§ndnis zu vertiefen ‚Äì auch bei korrekten Antworten 

Es erfolgt keine Bewertung oder Speicherung Ihrer Ergebnisse. Nutzen Sie dieses Assessment, um Wissensl√ºcken zu identifizieren und gegebenenfalls die entsprechenden Abschnitte des Kapitels noch einmal zu bearbeiten.

**Gesch√§tzte Zeit**: 45min

Viel Erfolg!
````

## Frage 1

```{code-cell} ipython3
:tags: [remove-input]
from jupyterquiz import display_quiz

"""
Lernziel: Sie k√∂nnen die Grundkonzepte des Natural Language Processing erkl√§ren und die Funktionen von Tokenisierung und Lemmatisierung f√ºr die Textanalyse beschreiben.
Bloom-Stufe: Verstehen
Format: Multiple Choice
"""


import sys
sys.path.append("..")
from quadriga import colors

multiple_choice_1 = [{
    "question": """Welche der folgenden Aussagen beschreibt korrekt, was NLP im Kontext der Textanalyse leistet?""",
    "type": "multiple_choice",
    "answers": [
        {
            "answer": "NLP dient ausschlie√ülich der automatischen √úbersetzung zwischen Sprachen",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Obwohl maschinelle √úbersetzung ein Anwendungsgebiet von NLP ist, umfasst NLP viel mehr, insbesondere die Anreicherung von Texten mit linguistischen Informationen f√ºr verschiedene Analysezwecke. NLP beinhaltet Methoden wie Tokenisierung, Lemmatisierung und viele weitere Techniken zur Textverarbeitung."""
        },
        {
            "answer": "NLP erm√∂glicht die Anreicherung von Texten mit linguistischen Informationen, die f√ºr Computer sonst nicht erkennbar w√§ren",
            "correct": True,
            "feedback": """‚úì Richtig! NLP (Natural Language Processing) erm√∂glicht es, Texte mit linguistischen Informationen anzureichern und damit f√ºr Computer "verst√§ndlich" zu machen. Computer sehen Text zun√§chst nur als Zeichenkette, w√§hrend NLP-Methoden semantische Strukturen hinzuf√ºgen und Analysen auf Wortebene erm√∂glichen."""
        },
        {
            "answer": "NLP ist ein Verfahren zur manuellen Annotation von Texten durch Sprachwissenschaftler:innen",
            "correct": False,
            "feedback": """√ó Nicht korrekt. NLP bezieht sich auf automatisierte Verfahren zur Textverarbeitung durch Computer, nicht auf manuelle Annotation. W√§hrend manuelle Annotation durchaus f√ºr Training und Evaluation von NLP-Systemen wichtig sein kann, ist NLP selbst ein computergest√ºtzter Prozess."""
        },
        {
            "answer": "NLP kann nur auf modernen Texten angewendet werden, nicht auf historischen Dokumenten",
            "correct": False,
            "feedback": """√ó Nicht korrekt. NLP kann grunds√§tzlich auf Texte jeder Epoche angewendet werden, auch wenn historische Texte aufgrund von Sprachver√§nderungen, anderen Schriften oder OCR-Qualit√§t besondere Herausforderungen darstellen k√∂nnen. Gerade in den Digital Humanities wird NLP h√§ufig f√ºr historische Textanalyse eingesetzt."""
        }
    ]
}]

display_quiz(multiple_choice_1, colors=colors.jupyterquiz)
```

## Frage 2

```{code-cell} ipython3
:tags: [remove-input]
from jupyterquiz import display_quiz

import sys
sys.path.append("..")
from quadriga import colors

multiple_choice_2 = [{
    "question": """Welche Funktion erf√ºllt die Tokenisierung im NLP-Prozess?""",
    "type": "multiple_choice",
    "answers": [
        {
            "answer": "Sie √ºbersetzt den Text in eine andere Sprache",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Die √úbersetzung ist eine komplexe NLP-Aufgabe, die weit √ºber die Tokenisierung hinausgeht. Tokenisierung ist ein grundlegender Vorverarbeitungsschritt, der Texte in kleinere Einheiten zerlegt, aber keine √úbersetzung vornimmt."""
        },
        {
            "answer": "Sie identifiziert die Stimmung (positiv/negativ) eines Textes",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage beschreibt nicht die Tokenisierung, sondern die Sentimentanalyse, die eine fortgeschrittene NLP-Aufgabe ist. Tokenisierung ist ein Vorverarbeitungsschritt, der f√ºr die Sentimentanalyse notwendig sein kann, aber selbst keine emotionale Bewertung vornimmt."""
        },
        {
            "answer": "Sie zerlegt einen Text in einzelne, sinnvolle Einheiten wie W√∂rter und Satzzeichen",
            "correct": True,
            "feedback": """‚úì Richtig! Die Tokenisierung ist der Prozess der Zerlegung eines Textes in einzelne, bedeutungstragende Einheiten (Token). Diese k√∂nnen W√∂rter, Zahlen oder Satzzeichen sein. Sie ist der erste grundlegende Schritt in der NLP-Pipeline, auf dem weitere Verarbeitungsschritte aufbauen."""
        },
        {
            "answer": "Sie wandelt alle W√∂rter in ihre Grundform um",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage beschreibt nicht die Tokenisierung, sondern die Lemmatisierung. Die Tokenisierung zerlegt Text in einzelne Einheiten, w√§hrend die Lemmatisierung in einem sp√§teren Schritt diese Einheiten auf ihre Grundform zur√ºckf√ºhrt."""
        }
    ]
}]

display_quiz(multiple_choice_2, colors=colors.jupyterquiz)
```

## Frage 3

```{code-cell} ipython3
:tags: [remove-input]
from jupyterquiz import display_quiz

import sys
sys.path.append("..")
from quadriga import colors

multiple_choice_3 = [{
    "question": """Was ist das Hauptziel der Lemmatisierung in der NLP-Pipeline?""",
    "type": "multiple_choice",
    "answers": [
        {
            "answer": "Die Bestimmung der grammatikalischen Funktion eines Wortes im Satz",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage beschreibt nicht die Lemmatisierung, sondern das Part-of-Speech Tagging (POS-Tagging). Die Lemmatisierung befasst sich mit der Reduktion von Wortformen auf ihre Grundform, nicht mit der grammatikalischen Funktionsanalyse."""
        },
        {
            "answer": "Die Zur√ºckf√ºhrung verschiedener Wortformen auf ihre gemeinsame Grundform",
            "correct": True,
            "feedback": """‚úì Richtig! Die Lemmatisierung f√ºhrt verschiedene Formen eines Wortes auf ihre gemeinsame Grundform (Lemma) zur√ºck. Beispielsweise werden "gehe", "gehst", "geht", "ging" auf "gehen" zur√ºckgef√ºhrt. Dies vereinfacht die Textanalyse, da verschiedene Formen desselben Wortes als eine Einheit betrachtet werden k√∂nnen."""
        },
        {
            "answer": "Die Erkennung von Eigennamen und geographischen Bezeichnungen",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage beschreibt nicht die Lemmatisierung, sondern die Named Entity Recognition (NER). Die Lemmatisierung hat nicht das Ziel, bestimmte semantische Kategorien von W√∂rtern zu identifizieren, sondern Wortformen zu vereinheitlichen."""
        },
        {
            "answer": "Die Zerlegung von Komposita in ihre Bestandteile",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage beschreibt nicht die prim√§re Funktion der Lemmatisierung. Die Zerlegung von Komposita (zusammengesetzten W√∂rtern) ist eine separate NLP-Aufgabe, die als Compound Splitting bezeichnet wird. Die Lemmatisierung befasst sich mit der Reduktion von flektierten Formen auf ihre Grundform."""
        }
    ]
}]

display_quiz(multiple_choice_3, colors=colors.jupyterquiz)
```

## Frage 4

```{code-cell} ipython3
:tags: [remove-input]
from jupyterquiz import display_quiz

import sys
sys.path.append("..")
from quadriga import colors

multiple_choice_4 = [{
    "question": """Warum ist die Kombination von Tokenisierung und Lemmatisierung f√ºr die quantitative Textanalyse besonders wertvoll?""",
    "type": "multiple_choice",
    "answers": [
        {
            "answer": "Sie erm√∂glicht die automatische √úbersetzung des Textes",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Obwohl Tokenisierung und Lemmatisierung grundlegende Schritte in der √úbersetzungspipeline sein k√∂nnen, reichen sie allein nicht aus, um eine automatische √úbersetzung zu erm√∂glichen. Daf√ºr sind komplexere Modelle notwendig."""
        },
        {
            "answer": "Sie verbessert die Lesbarkeit des Textes f√ºr Menschen",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Tokenisierung und Lemmatisierung sind in erster Linie f√ºr die maschinelle Verarbeitung gedacht, nicht f√ºr die menschliche Lesbarkeit. Im Gegenteil, ein lemmatisierter Text kann f√ºr Menschen sogar schwerer zu lesen sein, da er nicht mehr den gewohnten grammatikalischen Formen entspricht."""
        },
        {
            "answer": "Sie erm√∂glicht pr√§zisere H√§ufigkeitsanalysen durch Normalisierung verschiedener Wortformen",
            "correct": True,
            "feedback": """‚úì Richtig! Die Kombination von Tokenisierung und Lemmatisierung ist f√ºr quantitative Textanalysen besonders wertvoll, weil sie die Varianz der Wortformen reduziert. Dadurch k√∂nnen H√§ufigkeitsanalysen pr√§ziser durchgef√ºhrt werden, da verschiedene Formen desselben Wortes nicht mehr als verschiedene W√∂rter gez√§hlt werden."""
        },
        {
            "answer": "Sie korrigiert Rechtschreibfehler im Originaltext",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Weder Tokenisierung noch Lemmatisierung sind prim√§r darauf ausgerichtet, Rechtschreibfehler zu korrigieren. Rechtschreibkorrektur ist eine separate NLP-Aufgabe, die vor der Tokenisierung und Lemmatisierung durchgef√ºhrt werden sollte."""
        }
    ]
}]

display_quiz(multiple_choice_4, colors=colors.jupyterquiz)
```

## Frage 5

```{code-cell} ipython3
:tags: [remove-input]
from jupyterquiz import display_quiz

import sys
sys.path.append("..")
from quadriga import colors

multiple_choice_5 = [{
    "question": """Welcher Unterschied besteht zwischen dem urspr√ºnglichen Text und dem mit NLP-Methoden verarbeiteten Text?""",
    "type": "multiple_choice",
    "answers": [
        {
            "answer": "Der verarbeitete Text enth√§lt keine Satzzeichen mehr",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage ist nicht allgemein zutreffend. Ob Satzzeichen im verarbeiteten Text erhalten bleiben, h√§ngt von den spezifischen NLP-Methoden und deren Konfiguration ab. In vielen NLP-Pipelines werden Satzzeichen als eigene Token behandelt und bleiben somit erhalten."""
        },
        {
            "answer": "Der verarbeitete Text ist in eine andere Sprache √ºbersetzt",
            "correct": False,
            "feedback": """√ó Nicht korrekt. NLP-Methoden umfassen viele verschiedene Verarbeitungsschritte, von denen √úbersetzung nur einer ist. Die meisten grundlegenden NLP-Verarbeitungsschritte wie Tokenisierung und Lemmatisierung ver√§ndern die Sprache des Textes nicht."""
        },
        {
            "answer": "Der verarbeitete Text ist mit zus√§tzlichen linguistischen Informationen angereichert",
            "correct": True,
            "feedback": """‚úì Richtig! Der wesentliche Unterschied besteht darin, dass der mit NLP-Methoden verarbeitete Text mit zus√§tzlichen linguistischen Informationen angereichert ist. Dies k√∂nnen Informationen √ºber Wortarten, Grundformen, syntaktische Beziehungen oder semantische Bedeutungen sein. Der Computer kann dadurch auf verschiedenen linguistischen Ebenen mit dem Text arbeiten."""
        },
        {
            "answer": "Der verarbeitete Text ist k√ºrzer als der Originaltext",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage ist nicht allgemein zutreffend. Die L√§nge des verarbeiteten Textes h√§ngt von den angewendeten NLP-Methoden ab. Bei manchen Methoden (wie z.B. Zusammenfassung) kann der Text k√ºrzer werden, bei anderen (wie z.B. Annotation) kann er durch zus√§tzliche Informationen sogar l√§nger werden."""
        }
    ]
}]

display_quiz(multiple_choice_5, colors=colors.jupyterquiz)
```

## Frage 6
Analysieren Sie den folgenden Satz mit NLP-Methoden und beschreiben Sie die Ergebnisse:

Originaltext: "Die Forschenden untersuchten verschiedene Zeitungsartikel zur Spanischen Grippe."

1.	F√ºhren Sie eine Tokenisierung durch.
2.	Bestimmen Sie die Lemmata der einzelnen Token.
3.	Reflektieren Sie, wie diese Verarbeitung eine quantitative Analyse unterst√ºtzen w√ºrde.


```{code-cell} ipython3
:tags: [remove-input]
import sys
sys.path.append("../quadriga")  # Adjust path as needed
from assessment import create_answer_box

create_answer_box('process-1')
```

````{admonition} L√∂sung
:class: solution, dropdown

**Beispiell√∂sung zur Selbstbewertung:**
1.	Tokenisierung: ["Die", "Forschenden", "untersuchten", "verschiedene", "Zeitungsartikel", "zur", "Spanischen", "Grippe", "."]
2.	Lemmatisierung: ["der", "Forschende", "untersuchen", "verschieden", "Zeitungsartikel", "zu", "spanisch", "Grippe", "."]

**Reflexion:**
- Die Tokenisierung erm√∂glicht die Analyse auf Wortebene und bereitet den Text f√ºr weitere Verarbeitung vor
- Die Lemmatisierung w√ºrde alle Formen von "untersuchen" zusammenfassen, was bei einer Frequenzanalyse hilfreich ist
- Durch die Lemmatisierung werden verschiedene Flexionsformen (wie "Spanischen" zu "spanisch") vereinheitlicht ('die' wird standardm√§√üig zu 'der' lemmatisiert).
- Bei einer gr√∂√üeren Textsammlung w√ºrden verschiedene grammatikalische Formen desselben Wortes nicht als unterschiedliche Begriffe gez√§hlt
- Diese Normalisierung verbessert die Qualit√§t von H√§ufigkeitsanalysen, Keyword-Extraktion und thematischen Analysen
- Die Informationen √ºber die urspr√ºngliche Form bleiben erhalten und k√∂nnen f√ºr detailliertere linguistische Analysen genutzt werden

**Bewertungskriterien f√ºr die Selbsteinsch√§tzung:**
- Korrekte Tokenisierung mit Ber√ºcksichtigung von Satzzeichen als eigene Token
- Korrekte Bestimmung der Grundformen bei der Lemmatisierung
- Verst√§ndnis des Nutzens f√ºr quantitative Textanalyse, insbesondere f√ºr H√§ufigkeitsanalysen
- Reflexion √ºber die Vor- und Nachteile der Methoden f√ºr die spezifische Forschungsfrage

````



## Frage 7
(W√§hlen Sie alle zutreffenden Antworten aus)

```{code-cell} ipython3
:tags: [remove-input]
from jupyterquiz import display_quiz

"""
Lernziel: Sie k√∂nnen die notwendigen Schritte zur automatischen Annotation eines Texts aufz√§hlen und Vorteile der Tokenisierung gegen√ºber einfacheren Methoden der Worttrennung nennen.
Bloom-Stufe: Verstehen
Format: Multiple Choice
"""

import sys
sys.path.append("..")
from quadriga import colors

multiple_choice_1 = [{
    "question": """Welche Schritte geh√∂ren zur automatischen Annotation eines Textes mit spaCy?""",
    "type": "multiple_choice",
    "answers": [
        {
            "answer": "Einlesen des Textes",
            "correct": True,
            "feedback": """‚úì Richtig! Der erste Schritt im Notebook ist das Einlesen des Textes mit text_path.read_text(), nachdem der Pfad zur Datei definiert wurde (text_path = Path("../data/txt/SNP2719372X-19181015-0-0-0-0.txt"))."""
        },
        {
            "answer": "Laden des sprachspezifischen Modells",
            "correct": True,
            "feedback": """‚úì Richtig! Im Notebook wird das deutsche Sprachmodell mit nlp = spacy.load('de_core_news_sm') geladen, welches f√ºr die Verarbeitung deutscher Texte optimiert ist."""
        },
        {
            "answer": "Auswahl der relevanten Analysekomponenten",
            "correct": True,
            "feedback": """‚úì Richtig! Im Notebook werden nicht ben√∂tigte Komponenten deaktiviert: disable_components = ['ner', 'morphologizer', 'attribute_ruler', 'sentencizer'], um die Verarbeitungsgeschwindigkeit zu erh√∂hen. Dieser Schritt kann bei geringer Textmenge √ºbersprungen werden."""
        },
        {
            "answer": "Manuelle Korrektur der Tokenisierungsfehler",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Die Annotation mit spaCy erfolgt vollautomatisch. Im Notebook gibt es keinen Schritt zur manuellen Korrektur von Tokenisierungsfehlern."""
        },
        {
            "answer": "Speichern der Ergebnisse in einem strukturierten Format",
            "correct": True,
            "feedback": """‚úì Richtig! Im letzten Schritt werden die Ergebnisse als CSV-Datei gespeichert: text_annotated_df.to_csv(output_path, index=False). Das Notebook erw√§hnt, dass CSV das Standardformat ist, um tabellarische Daten im Klartext zu speichern."""
        }
    ]
}]

display_quiz(multiple_choice_1, colors=colors.jupyterquiz)
```

## Frage 8
(W√§hlen Sie alle zutreffenden Antworten aus)

```{code-cell} ipython3
:tags: [remove-input]
from jupyterquiz import display_quiz

import sys
sys.path.append("..")
from quadriga import colors

multiple_choice_2 = [{
    "question": """Welche Vorteile bietet die Tokenisierung mit spaCy gegen√ºber einer einfachen Worttrennung mittels split()?""",
    "type": "multiple_choice",
    "answers": [
        {
            "answer": "Satzzeichen werden als eigene Token erkannt",
            "correct": True,
            "feedback": """‚úì Richtig! Das Notebook zeigt im Beispiel words[100:120], dass bei der einfachen Teilung mit split() Satzzeichen an W√∂rtern h√§ngen bleiben (z.B. "Entfernung,", "anzugreisen."), w√§hrend spaCy sie als separate Token erkennt."""
        },
        {
            "answer": "Die Gesamtanzahl der Token ist akkurater",
            "correct": True,
            "feedback": """‚úì Richtig! Im Notebook wird explizit darauf hingewiesen: "Wie zu sehen ist, hat diese Art der 'falschen' Tokenisierung den Nachteil, dass Satzzeichen nicht von W√∂rtern abgetrennt werden. Die Wortanzahl ist dementsprechend auch nicht akkurat." Der Vergleich zeigt 11.612 W√∂rter mit split() gegen√ºber 15.062 Token mit spaCy."""
        },
        {
            "answer": "Die Tokenisierung mit spaCy ist immer schneller",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage wird nicht durch das Notebook unterst√ºtzt. Im Gegenteil, das Notebook zeigt, dass die spaCy-Annotation 1,47 Sekunden dauert, und f√ºr gr√∂√üere Textmengen werden sogar Optimierungen vorgeschlagen, indem nicht ben√∂tigte Komponenten deaktiviert werden."""
        },
        {
            "answer": "Sie erm√∂glicht die Lemmatisierung der Tokens",
            "correct": False,
            "feedback": """x Nicht korrekt: Auch wenn im Notebook die Tokenisierung und die Lemmatisierung im selben Schritt erfolgen, ist die Lemmatisierung kein Teil der Tokenisierung, sondern erfolgt aufbauend auf dieser.""" 
        },
        {
            "answer": "Sie korrigiert automatisch OCR-Fehler im Text",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage wird nicht durch das Notebook unterst√ºtzt. spaCy f√ºhrt keine automatische Korrektur von OCR-Fehlern durch. Die Qualit√§t der Annotation h√§ngt von der Qualit√§t des Eingabetextes ab."""
        }
    ]
}]

display_quiz(multiple_choice_2, colors=colors.jupyterquiz)
```

## Frage 9

```{code-cell} ipython3
:tags: [remove-input]
from jupyterquiz import display_quiz

import sys
sys.path.append("..")
from quadriga import colors

multiple_choice_3 = [{
    "question": """Warum werden im Beispiel bestimmte Analysekomponenten von spaCy deaktiviert?""",
    "type": "multiple_choice",
    "answers": [
        {
            "answer": "Um die Genauigkeit der Tokenisierung zu erh√∂hen",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage wird nicht durch das Notebook unterst√ºtzt. Das Deaktivieren von Komponenten dient nicht der Verbesserung der Genauigkeit."""
        },
        {
            "answer": "Um die Verarbeitungsgeschwindigkeit zu erh√∂hen",
            "correct": True,
            "feedback": """‚úì Richtig! Im Notebook steht explizit: "Es werden einige Analysekomponent wie z. B. das Aufteilen des Texts in S√§tze (sentencizer) oder die Named Entity Recognition (ner) ausgeschlossen, da diese f√ºr die Tokenisierung und die Lemmatisierung nicht ben√∂tigt werden. Der Auschluss der Komponentnen erh√∂ht die Annotationsgeschwindikgeit." """
        },
        {
            "answer": "Um mehr Speicherplatz f√ºr die Ergebnisse zu haben",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage wird nicht durch das Notebook unterst√ºtzt. Der Speicherplatz f√ºr die Ergebnisse wird nicht als Grund f√ºr die Deaktivierung von Komponenten genannt."""
        },
        {
            "answer": "Um Kompatibilit√§tsprobleme mit dem CSV-Format zu vermeiden",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage wird nicht durch das Notebook unterst√ºtzt. Es gibt keinen Hinweis darauf, dass die Deaktivierung von Komponenten mit dem CSV-Format zusammenh√§ngt."""
        }
    ]
}]

display_quiz(multiple_choice_3, colors=colors.jupyterquiz)
```

## Frage 10

```{code-cell} ipython3
:tags: [remove-input]
from jupyterquiz import display_quiz

import sys
sys.path.append("..")
from quadriga import colors

multiple_choice_4 = [{
    "question": """Welches Dateiformat wird im Beispiel f√ºr die Speicherung der annotierten Texte verwendet und warum?""",
    "type": "multiple_choice",
    "answers": [
        {
            "answer": "TXT, weil es am wenigsten Speicherplatz ben√∂tigt",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Laut Notebook werden die annotierten Texte nicht als TXT-Dateien gespeichert."""
        },
        {
            "answer": "CSV, weil es gut f√ºr die Speicherung tabellarischer Daten geeignet ist",
            "correct": True,
            "feedback": """‚úì Richtig! Im Notebook steht explizit: "CSV (comma-separated value) ist das Standardformat um tabellarische Daten im Klartext zu speichern." Au√üerdem wird erw√§hnt: "Das Tabellenformat wurde gew√§hlt, da sich darin gut relationale Daten speichern lassen." """
        },
        {
            "answer": "JSON, weil es die hierarchische Struktur der Annotation am besten abbildet",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage ist nicht korrekt, da die Annotationen nicht hierarchisch strukturiert sind. Im Notebook wird JSON nicht als Speicherformat erw√§hnt, ist aber auch ein m√∂gliches Format, um annotierte Daten zu speichern. """
        },
        {
            "answer": "XML, weil es von den meisten Textanalysewerkzeugen unterst√ºtzt wird",
            "correct": False,
            "feedback": """√ó Nicht korrekt. XML ist in den Digital Humanities zwar ein Standardformat, um Annotationen zu speichern, allerdings sind nicht die meisten Analysewerkzeuge auf das Format ausgelegt. XML eignet sich f√ºr die Speicherung von komplexeren Annotationen oder Textauszeichnungen besser."""
        }
    ]
}]

display_quiz(multiple_choice_4, colors=colors.jupyterquiz)
```

## Frage 11

```{code-cell} ipython3
:tags: [remove-input]
from jupyterquiz import display_quiz

import sys
sys.path.append("..")
from quadriga import colors

multiple_choice_5 = [{
    "question": """Was zeigt der Vergleich der Textl√§nge vor und nach der Tokenisierung mit spaCy im Beispiel?""",
    "type": "multiple_choice",
    "answers": [
        {
            "answer": "Der tokenisierte Text ist k√ºrzer, weil Stoppw√∂rter entfernt wurden",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage ist nicht korrekt, da es im Notebook keinen Hinweis darauf gibt, dass Stoppw√∂rter entfernt wurden. Im Gegenteil, die Tokenzahl erh√∂ht sich."""
        },
        {
            "answer": "Der tokenisierte Text enth√§lt mehr Elemente, weil Satzzeichen als eigene Token erkannt werden",
            "correct": True,
            "feedback": """‚úì Richtig! Das Notebook zeigt: "Durch die Tokenisierung wurden z. B. Satzzeichen von W√∂rtern abgetrennt. An der Textl√§nge l√§sst sich dies schon erkennen." Die Anzahl steigt von 11.612 auf 15.062 Token."""
        },
        {
            "answer": "Der tokenisierte Text ist unver√§ndert in seiner L√§nge, nur die Qualit√§t der Tokens ist verbessert",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage widerspricht dem im Notebook gezeigten Unterschied in der Tokenanzahl (11.612 vs. 15.062)."""
        },
        {
            "answer": "Der tokenisierte Text ist k√ºrzer, weil zusammengeh√∂rige Mehrwortausdr√ºcke als ein Token erkannt werden",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Aussage widerspricht dem im Notebook gezeigten Anstieg der Tokenanzahl."""
        }
    ]
}]

display_quiz(multiple_choice_5, colors=colors.jupyterquiz)
```

## Frage 12
Ordnen Sie die Schritte in die richtige Reihenfolge, um einen Text mit spaCy zu annotieren:

1. Speichern der Annotation als CSV-Datei
2. Auswahl der Analysekomponenten
3. Laden des Sprachmodells
4. Einlesen des Textes
5. Durchf√ºhrung der Annotation mit nlp(text)

```{code-cell} ipython3
:tags: [remove-input]
from jupyterquiz import display_quiz

import sys
sys.path.append("..")
from quadriga import colors

multiple_choice_6 = [{
    "question": """Welche Reihenfolge der Verarbeitungsschritte ist korrekt f√ºr die Textannotation mit spaCy?""",
    "type": "multiple_choice",
    "answers": [
        {
            "answer": "3 ‚Üí 2 ‚Üí 4 ‚Üí 5 ‚Üí 1",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Reihenfolge ist nicht korrekt. Laut Notebook wird zuerst der Text eingelesen (Schritt 1), bevor weitere Verarbeitungsschritte folgen."""
        },
        {
            "answer": "3 ‚Üí 4 ‚Üí 2 ‚Üí 5 ‚Üí 1",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Diese Reihenfolge stimmt nicht mit dem Ablauf im Notebook √ºberein, in dem zuerst der Text eingelesen wird."""
        },
        {
            "answer": "4 ‚Üí 3 ‚Üí 2 ‚Üí 5 ‚Üí 1",
            "correct": True,
            "feedback": """‚úì Richtig! Die Reihenfolge im Notebook ist: 1. Einlesen des Texts, 2. Laden des Sprachmodells, 3. Auswahl der Analysekomponenten, 4. Durchf√ºhrung der Annotation, 5. Speichern der Ergebnisse."""
        },
        {
            "answer": "4 ‚Üí 5 ‚Üí 3 ‚Üí 2 ‚Üí 1",
            "correct": False,
            "feedback": """√ó Nicht korrekt. Das Sprachmodell muss vor der Annotation geladen werden, wie im Notebook gezeigt."""
        }
    ]
}]

display_quiz(multiple_choice_6, colors=colors.jupyterquiz)
```
